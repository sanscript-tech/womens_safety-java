# -*- coding: utf-8 -*-
"""Image-Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1StOyfUgZVCPmQWyAJCvGsMwtpe69Uky3
"""

#Loading the necessary libraries
import os
import numpy as np
import pandas as pd
import torch 
import glob
import torch.nn as nn
from torchvision.transforms import transforms
from torch.utils.data import DataLoader
from torch.optim import Adam
from torch.autograd import Variable
import torchvision
import pathlib

device = torch.device("cuda" if torch.cuda.is_available() 
                                  else "cpu")

"""Data Preprocessing"""

transformer = transforms.Compose([
                                  transforms.Resize((150,150)),
                                  transforms.RandomHorizontalFlip(),
                                  transforms.ToTensor(), #0-255 will be changed to 0-1 and datatype will be changed from numpy to tensor.
                                  transforms.Normalize([0.5,0.5,0.5],
                                                       [0.5,0.5,0.5])
                                ])

from google.colab import drive

drive.mount('/content/gdrive')

#dataload to feed in the dataset in batch sizes.
#Path for training and testing dataset.

train_path = '/content/gdrive/My Drive/Image Classification Dataset/archive/seg_train/seg_train'
test_path = '/content/gdrive/My Drive/Image Classification Dataset/archive/seg_test/seg_test'

train_loader = DataLoader(torchvision.datasets.ImageFolder(train_path, transform=transformer),batch_size=256, shuffle=True)

test_loader = DataLoader(torchvision.datasets.ImageFolder(test_path, transform=transformer),batch_size=256, shuffle=True)

#categories

root=pathlib.Path(train_path)
classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])
print(classes)

"""**CNN Class**"""

class ConvNet(nn.Module):
  def __init__(self,num_classes=6):
    super(ConvNet,self).__init__()
  
    self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)
        #Shape= (256,12,150,150)
    self.bn1=nn.BatchNorm2d(num_features=12)
        #Shape= (256,12,150,150)
    self.relu1=nn.ReLU()
        #Shape= (256,12,150,150)
        
    self.pool=nn.MaxPool2d(kernel_size=2)
        #Reduce the image size be factor 2
        #Shape= (256,12,75,75)
        
        
    self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)
        #Shape= (256,20,75,75)
    self.relu2=nn.ReLU()
        #Shape= (256,20,75,75)
        
        
        
    self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)
        #Shape= (256,32,75,75)
    self.bn3=nn.BatchNorm2d(num_features=32)
        #Shape= (256,32,75,75)
    self.relu3=nn.ReLU()
        #Shape= (256,32,75,75)
        
        
    self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)

    #Feed Forward Function
  def forward(self,input):
    output = self.conv1(input)
    output = self.bn1(output)
    output = self.relu1(output)

    output=self.pool(output)

    output = self.conv2(output)
    output = self.relu2(output)

    output = self.conv3(output)
    output = self.bn3(output)
    output = self.relu3(output)

    #above output will be in a matrix form with size [256,32,75,75]
    output = output.view(-1,32*75*75) #reshaping into a matrix of size 32*75*75
    output=self.fc(output)

    return output

model = ConvNet(num_classes=6).to(device)

#Optimizer and loss function
optimizer = Adam(model.parameters(),lr=0.001, weight_decay=0.0001)

loss_function = nn.CrossEntropyLoss()

#Hyperparameter which can be tuned
num_epochs = 10

# #Calculating the size of training and testing images
train_count = len(glob.glob(train_path+'/**/*.jpg'))
test_count = len(glob.glob(test_path+'/**/*.jpg'))
print(train_count,test_count)

#Training of the Model and saving the best model

best_accuracy=0.0
for epoch in range(num_epochs):
  #Evaluation and training on train dataset
  model.train()
  train_accuracy=0.0
  train_loss=0.0

  for i, (images,labels) in enumerate(train_loader):
    if torch.cuda.is_available():
      images = Variable(images.cuda())
      labels = Variable(labels.cuda())
    
    optimizer.zero_grad() #zero the grads out after each batch to prevent mixing

    outputs = model(images)
    loss = loss_function(outputs,labels)
    loss.backward()
    optimizer.step()

    train_loss += loss.cpu().data*images.size(0)
    _,prediction = torch.max(outputs.data,1)

    train_accuracy += int(torch.sum(prediction==labels.data)) 

  train_accuracy = train_accuracy/train_count 
  train_loss = train_loss/train_count 

   #Evaluation on testing dataset
  model.eval()

  test_accuracy = 0.0
  for i, (images,labels) in enumerate(test_loader):
    if torch.cuda.is_availabe():
      images = Variable(images.cuda())
      labels = Variable(labels.cuda())
    
    outputs = model(images)
    _,predictions = torch.max(outputs.data,1)

    test_accuracy += int(torch.sum(predictions==labels.data))
  
  test_accuracy = test_accuracy/test_count 
  
  print("Epoch: "+str(epoch)+ " Train loss: "+str(int(train_loss))+" Train accuracy: "+str(train_accuracy)+" Test Accuracy: "+str(test_accuracy))

  #Saving the results for the best Model
  if test_accuracy>best_accuracy:
    torch.save(model.state_dict(),'best_checkpoint_model')
    best_accuracy = test_accuracy

